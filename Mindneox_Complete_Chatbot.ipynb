{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23fd2377",
   "metadata": {},
   "source": [
    "# ğŸ¤– Mindneox.ai Chatbot - FREE GPU Version\n",
    "\n",
    "**Complete conversational AI with Pinecone storage**\n",
    "\n",
    "âš¡ 10x faster than Mac M1/M2  \n",
    "ğŸ’° 100% FREE on Google Colab  \n",
    "ğŸ§  Powered by Mistral-7B  \n",
    "ğŸ—„ï¸ Stores all data in Pinecone\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ Instructions:\n",
    "1. **Enable GPU**: Runtime â†’ Change runtime type â†’ GPU â†’ Save\n",
    "2. **Run all cells**: Runtime â†’ Run all (or Ctrl+F9)\n",
    "3. **Start chatting** when Cell 7 finishes!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f764143b",
   "metadata": {},
   "source": [
    "## ğŸ”¥ CELL 1: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c627844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ¤– Mindneox.ai Chatbot - GPU Setup\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nCUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(\"\\nğŸ‰ FREE GPU READY FOR CHATBOT!\")\n",
    "else:\n",
    "    print(\"\\nâŒ Enable GPU: Runtime â†’ Change runtime type â†’ GPU â†’ Save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011dadc1",
   "metadata": {},
   "source": [
    "## ğŸ“¦ CELL 2: Install Packages (3 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d175712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“¦ Installing chatbot dependencies...\")\n",
    "print(\"â±ï¸  Takes about 3 minutes\\n\")\n",
    "\n",
    "!pip install -q llama-cpp-python langchain langchain-core langchain-community\n",
    "!pip install -q sentence-transformers transformers accelerate\n",
    "!pip install -q pinecone-client redis\n",
    "\n",
    "print(\"\\nâœ… All packages installed!\")\n",
    "print(\"âœ… Ready to build chatbot with Pinecone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2cd44e",
   "metadata": {},
   "source": [
    "## ğŸ“¥ CELL 3: Download Mistral-7B Model (3 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c36a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "MODEL_FILE = \"Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\"\n",
    "MODEL_URL = \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\"\n",
    "\n",
    "if not os.path.exists(MODEL_FILE):\n",
    "    print(f\"ğŸ“¥ Downloading chatbot brain (4.37 GB)...\")\n",
    "    print(\"â±ï¸  Takes about 2-3 minutes\\n\")\n",
    "    !wget --show-progress {MODEL_URL}\n",
    "    \n",
    "    if os.path.exists(MODEL_FILE):\n",
    "        size = os.path.getsize(MODEL_FILE) / 1024**3\n",
    "        print(f\"\\nâœ… Downloaded! Size: {size:.2f} GB\")\n",
    "    else:\n",
    "        print(\"\\nâŒ Download failed. Try: Runtime â†’ Restart runtime\")\n",
    "else:\n",
    "    size = os.path.getsize(MODEL_FILE) / 1024**3\n",
    "    print(f\"âœ… Chatbot brain ready! Size: {size:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7684b457",
   "metadata": {},
   "source": [
    "## ğŸ—„ï¸ CELL 4: Connect to Pinecone Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033ecdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import os\n",
    "\n",
    "# Your Pinecone API key\n",
    "PINECONE_API_KEY = \"pcsk_4oPVPT_PXLxHyGVPeKAjYJLf7VwPG1Kq1YoNQGqXxzp62hPaYW9yt8Vs3uCYd1xqA4bFqn\"\n",
    "INDEX_NAME = \"mindnex-responses\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ—„ï¸  Connecting to Pinecone Cloud Database\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Initialize Pinecone\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    \n",
    "    # Check if index exists\n",
    "    existing_indexes = [index.name for index in pc.list_indexes()]\n",
    "    \n",
    "    if INDEX_NAME in existing_indexes:\n",
    "        index = pc.Index(INDEX_NAME)\n",
    "        stats = index.describe_index_stats()\n",
    "        vector_count = stats.get('total_vector_count', 0)\n",
    "        print(f\"\\nâœ… Connected to Pinecone!\")\n",
    "        print(f\"âœ… Index: {INDEX_NAME}\")\n",
    "        print(f\"âœ… Vectors stored: {vector_count}\")\n",
    "        print(f\"âœ… Ready to collect conversation data!\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  Index '{INDEX_NAME}' not found\")\n",
    "        print(\"Creating new index...\")\n",
    "        pc.create_index(\n",
    "            name=INDEX_NAME,\n",
    "            dimension=384,\n",
    "            metric='cosine',\n",
    "            spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
    "        )\n",
    "        index = pc.Index(INDEX_NAME)\n",
    "        print(f\"âœ… Created new index: {INDEX_NAME}\")\n",
    "    \n",
    "    pinecone_enabled = True\n",
    "    print(\"\\nâœ… Pinecone ready to store all conversations!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Pinecone connection failed: {e}\")\n",
    "    print(\"âš ï¸  Chatbot will work but won't save to database\")\n",
    "    pinecone_enabled = False\n",
    "    index = None\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a030972",
   "metadata": {},
   "source": [
    "## ğŸ§  CELL 5: Load AI Model on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2397e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ§  Loading Chatbot AI on FREE GPU\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load model with GPU acceleration\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\",\n",
    "    n_ctx=8192,\n",
    "    n_threads=2,\n",
    "    n_gpu_layers=-1,\n",
    "    n_batch=512,\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    repeat_penalty=1.2,\n",
    "    max_tokens=500,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Chatbot AI loaded on GPU!\")\n",
    "print(\"âœ… Ready for conversations!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578626eb",
   "metadata": {},
   "source": [
    "## ğŸ¤– CELL 6: Create Chatbot Class with Memory + Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a698499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import hashlib\n",
    "\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "class MindneoxChatbot:\n",
    "    def __init__(self, llm, pinecone_index=None):\n",
    "        self.llm = llm\n",
    "        self.conversation_history = []\n",
    "        self.start_time = datetime.now()\n",
    "        self.pinecone_index = pinecone_index\n",
    "        self.vectors_stored = 0\n",
    "        \n",
    "    def chat(self, user_message: str) -> str:\n",
    "        context = self._build_context()\n",
    "        full_prompt = f\"{context}\\n\\nUser: {user_message}\\nAssistant:\"\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke(full_prompt)\n",
    "            response = response.strip()\n",
    "            if response.startswith(\"Assistant:\"):\n",
    "                response = response[10:].strip()\n",
    "            \n",
    "            self.conversation_history.append({\n",
    "                'user': user_message,\n",
    "                'assistant': response,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            if pinecone_enabled and self.pinecone_index:\n",
    "                self._store_in_pinecone(user_message, response)\n",
    "            \n",
    "            return response\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    def _store_in_pinecone(self, user_msg: str, bot_response: str):\n",
    "        try:\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            unique_id = f\"chat_{timestamp}_{hashlib.md5(user_msg.encode()).hexdigest()[:8]}\"\n",
    "            combined_text = f\"User: {user_msg}\\nAssistant: {bot_response}\"\n",
    "            embedding = embedding_model.encode(combined_text).tolist()\n",
    "            \n",
    "            self.pinecone_index.upsert(vectors=[{\n",
    "                'id': unique_id,\n",
    "                'values': embedding,\n",
    "                'metadata': {\n",
    "                    'user_message': user_msg,\n",
    "                    'bot_response': bot_response,\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    'source': 'google_colab_chat'\n",
    "                }\n",
    "            }])\n",
    "            self.vectors_stored += 1\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâš ï¸  Pinecone storage failed: {e}\")\n",
    "    \n",
    "    def _build_context(self) -> str:\n",
    "        context = \"[INST] You are Mindneox.ai, a helpful AI assistant. You have conversations with users and remember previous messages.\\n\\n\"\n",
    "        recent = self.conversation_history[-5:]\n",
    "        for msg in recent:\n",
    "            context += f\"User: {msg['user']}\\n\"\n",
    "            context += f\"Assistant: {msg['assistant']}\\n\\n\"\n",
    "        context += \"[/INST]\"\n",
    "        return context\n",
    "    \n",
    "    def get_history(self):\n",
    "        return self.conversation_history\n",
    "    \n",
    "    def clear_history(self):\n",
    "        self.conversation_history = []\n",
    "        print(\"âœ… Conversation history cleared\")\n",
    "    \n",
    "    def get_stats(self):\n",
    "        return {\n",
    "            'total_messages': len(self.conversation_history),\n",
    "            'vectors_stored': self.vectors_stored,\n",
    "            'session_duration': str(datetime.now() - self.start_time).split('.')[0],\n",
    "            'messages_per_minute': len(self.conversation_history) / max(1, (datetime.now() - self.start_time).total_seconds() / 60)\n",
    "        }\n",
    "\n",
    "chatbot = MindneoxChatbot(llm, pinecone_index=index if pinecone_enabled else None)\n",
    "\n",
    "print(\"\\nâœ… Chatbot initialized with memory!\")\n",
    "if pinecone_enabled:\n",
    "    print(\"âœ… Pinecone storage ENABLED - All chats will be saved!\")\n",
    "else:\n",
    "    print(\"âš ï¸  Pinecone storage DISABLED\")\n",
    "print(\"âœ… Ready to chat!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d4de21",
   "metadata": {},
   "source": [
    "## ğŸ’¬ CELL 7: Interactive Chat - START HERE!\n",
    "\n",
    "**Commands:**\n",
    "- Type your message to chat\n",
    "- `history` - View conversation\n",
    "- `stats` - See statistics\n",
    "- `clear` - Clear history\n",
    "- `quit` - Exit chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a48e532",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ğŸ’¬ MINDNEOX.AI CHATBOT - Interactive Mode\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nğŸ¤– Hi! I'm Mindneox.ai, your AI assistant powered by FREE GPU!\")\n",
    "if pinecone_enabled:\n",
    "    print(\"âœ… Pinecone enabled - All conversations will be saved!\")\n",
    "print(\"\\nğŸ“ Commands: history | stats | clear | quit\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nğŸ˜Š You: \").strip()\n",
    "    \n",
    "    if not user_input:\n",
    "        continue\n",
    "    \n",
    "    if user_input.lower() == 'quit':\n",
    "        stats = chatbot.get_stats()\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ğŸ“Š Session Summary:\")\n",
    "        print(f\"   Messages: {stats['total_messages']}\")\n",
    "        if pinecone_enabled:\n",
    "            print(f\"   âœ… Saved to Pinecone: {stats['vectors_stored']}\")\n",
    "        print(f\"   Duration: {stats['session_duration']}\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"\\nğŸ‘‹ Thanks for chatting! Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    elif user_input.lower() == 'history':\n",
    "        history = chatbot.get_history()\n",
    "        if history:\n",
    "            print(\"\\nğŸ“œ Conversation History:\")\n",
    "            for i, msg in enumerate(history, 1):\n",
    "                print(f\"\\n{i}. You: {msg['user']}\")\n",
    "                print(f\"   Bot: {msg['assistant'][:100]}...\")\n",
    "        else:\n",
    "            print(\"\\nğŸ“œ No history yet\")\n",
    "        continue\n",
    "    \n",
    "    elif user_input.lower() == 'stats':\n",
    "        stats = chatbot.get_stats()\n",
    "        print(\"\\nğŸ“Š Statistics:\")\n",
    "        print(f\"   Messages: {stats['total_messages']}\")\n",
    "        if pinecone_enabled:\n",
    "            print(f\"   Pinecone: {stats['vectors_stored']}\")\n",
    "        print(f\"   Duration: {stats['session_duration']}\")\n",
    "        continue\n",
    "    \n",
    "    elif user_input.lower() == 'clear':\n",
    "        chatbot.clear_history()\n",
    "        continue\n",
    "    \n",
    "    print(\"\\nğŸ¤– Mindneox.ai: \", end=\"\", flush=True)\n",
    "    start = datetime.now()\n",
    "    response = chatbot.chat(user_input)\n",
    "    duration = (datetime.now() - start).total_seconds()\n",
    "    \n",
    "    print(response)\n",
    "    print(f\"\\nâš¡ {duration:.2f}s\", end=\"\")\n",
    "    if pinecone_enabled:\n",
    "        print(\" | âœ… Saved to Pinecone\")\n",
    "    else:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e936fdb",
   "metadata": {},
   "source": [
    "## ğŸ“Š CELL 8: Performance Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ec9c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“Š PERFORMANCE STATS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "stats = chatbot.get_stats()\n",
    "print(f\"\\nğŸ¤– Chatbot:\")\n",
    "print(f\"   Messages: {stats['total_messages']}\")\n",
    "print(f\"   Duration: {stats['session_duration']}\")\n",
    "\n",
    "if pinecone_enabled and index:\n",
    "    print(f\"\\nğŸ—„ï¸  Pinecone:\")\n",
    "    try:\n",
    "        index_stats = index.describe_index_stats()\n",
    "        total = index_stats.get('total_vector_count', 0)\n",
    "        print(f\"   Total vectors: {total}\")\n",
    "        print(f\"   This session: {stats['vectors_stored']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error: {e}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nğŸ”¥ GPU:\")\n",
    "    print(f\"   {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB used\")\n",
    "    print(f\"   Speed: 40-60 tokens/sec\")\n",
    "    print(f\"   Cost: $0 FREE!\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
