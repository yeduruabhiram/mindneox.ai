{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d14bea20",
   "metadata": {},
   "source": [
    "# üöÄ Mindneox.ai - Free GPU Testing on Google Colab\n",
    "\n",
    "**No installation needed on Mac! Run everything in your browser with FREE GPU.**\n",
    "\n",
    "## Setup Instructions:\n",
    "1. Click **Runtime** ‚Üí **Change runtime type** ‚Üí **GPU** ‚Üí **Save**\n",
    "2. Run each cell in order (Shift + Enter)\n",
    "3. Upload your files when prompted\n",
    "4. Test with free T4 GPU (10x faster than Mac!)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af136c8b",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU Availability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd7617b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2277d254",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2446101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üîç GPU Status Check\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check CUDA\n",
    "print(f\"\\n‚úÖ CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"‚úÖ CUDA Version: {torch.version.cuda}\")\n",
    "    print(\"\\nüéâ FREE GPU IS READY!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå GPU NOT ENABLED!\")\n",
    "    print(\"üí° Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU ‚Üí Save\")\n",
    "    print(\"Then restart this notebook.\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaf5458",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies (Takes ~3 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0626d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì¶ Installing dependencies...\")\n",
    "print(\"‚è±Ô∏è  This will take about 3 minutes\\n\")\n",
    "\n",
    "# Install PyTorch with CUDA (already included in Colab)\n",
    "!pip install -q llama-cpp-python langchain langchain-core langchain-community\n",
    "!pip install -q redis pinecone-client sentence-transformers\n",
    "!pip install -q transformers accelerate\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies installed!\")\n",
    "print(\"‚úÖ Ready for GPU acceleration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd68b3f7",
   "metadata": {},
   "source": [
    "## Step 3: Download Mistral-7B Model (Takes ~2 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afc684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_FILE = \"Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\"\n",
    "MODEL_URL = \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\"\n",
    "\n",
    "if not os.path.exists(MODEL_FILE):\n",
    "    print(f\"üì• Downloading {MODEL_FILE}...\")\n",
    "    print(\"‚è±Ô∏è  This will take about 2 minutes\\n\")\n",
    "    !wget -q --show-progress {MODEL_URL}\n",
    "    print(f\"\\n‚úÖ Model downloaded! Size: {os.path.getsize(MODEL_FILE) / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(f\"‚úÖ Model already exists! Size: {os.path.getsize(MODEL_FILE) / 1024**3:.2f} GB\")\n",
    "\n",
    "print(\"\\nüí° Model is ready for GPU inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a33657c",
   "metadata": {},
   "source": [
    "## Step 4: Setup Pinecone Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947e57a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "print(\"üîó Connecting to Pinecone...\")\n",
    "\n",
    "PINECONE_API_KEY = \"pcsk_5A9JjS_JVvYF7aE1kieuSnTXitm1pEMdVhg2wkpijQ3hiV9aC7rZ2CurG5qRfXE9FxHLAh\"\n",
    "INDEX_NAME = \"mindnex-responses\"\n",
    "\n",
    "try:\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    pinecone_index = pc.Index(INDEX_NAME)\n",
    "    \n",
    "    # Load embeddings on GPU\n",
    "    print(\"üî§ Loading embedding model on GPU...\")\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs={'device': 'cuda'}\n",
    "    )\n",
    "    \n",
    "    # Check stats\n",
    "    stats = pinecone_index.describe_index_stats()\n",
    "    print(f\"\\n‚úÖ Pinecone connected!\")\n",
    "    print(f\"‚úÖ Vectors stored: {stats['total_vector_count']}\")\n",
    "    print(f\"‚úÖ Embeddings on GPU: CUDA\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Pinecone error: {e}\")\n",
    "    pinecone_index = None\n",
    "    embeddings = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7961b041",
   "metadata": {},
   "source": [
    "## Step 5: Load Model with GPU Acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdb329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üöÄ Loading Mistral-7B with FREE GPU\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\",\n",
    "    n_ctx=8192,  # Large context\n",
    "    n_threads=2,\n",
    "    n_gpu_layers=-1,  # ALL layers on GPU\n",
    "    n_batch=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repeat_penalty=1.2,\n",
    "    max_tokens=500,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Model loaded on GPU!\")\n",
    "print(\"‚úÖ Using ALL model layers on T4 GPU\")\n",
    "print(\"‚úÖ Expected speed: 40-60 tokens/sec (10x faster than Mac!)\\n\")\n",
    "\n",
    "# Create prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"age\"],\n",
    "    template=\"[INST] Explain {topic} in detail for a {age} year old to understand. [/INST]\"\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3fe7d1",
   "metadata": {},
   "source": [
    "## Step 6: Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d24318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_in_pinecone(topic: str, response: str, age: str):\n",
    "    \"\"\"Store response in Pinecone with GPU-accelerated embeddings\"\"\"\n",
    "    if not pinecone_index or not embeddings:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nüíæ Storing in Pinecone...\")\n",
    "        \n",
    "        # Generate embedding on GPU\n",
    "        embedding = embeddings.embed_query(response)\n",
    "        \n",
    "        # Create unique ID\n",
    "        vector_id = f\"response_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{hash(topic) % 10000}\"\n",
    "        \n",
    "        # Metadata\n",
    "        metadata = {\n",
    "            'topic': topic,\n",
    "            'age': int(age) if age.isdigit() else 12,\n",
    "            'response': response,\n",
    "            'word_count': len(response.split()),\n",
    "            'character_count': len(response),\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'response_preview': response[:200],\n",
    "            'device': 'cuda',\n",
    "            'gpu_model': torch.cuda.get_device_name(0),\n",
    "            'platform': 'Google Colab Free'\n",
    "        }\n",
    "        \n",
    "        # Store in Pinecone\n",
    "        pinecone_index.upsert(\n",
    "            vectors=[{\n",
    "                'id': vector_id,\n",
    "                'values': embedding,\n",
    "                'metadata': metadata\n",
    "            }]\n",
    "        )\n",
    "        \n",
    "        print(f\"   ‚úÖ Stored in Pinecone: {vector_id}\")\n",
    "        print(f\"   ‚úÖ Embedding generated on GPU\")\n",
    "        return vector_id\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def generate_text(topic: str, age: str) -> tuple:\n",
    "    \"\"\"Generate text with GPU acceleration and timing\"\"\"\n",
    "    try:\n",
    "        print(f\"\\nüöÄ Generating response for: {topic}\")\n",
    "        print(f\"üë§ Target age: {age}\")\n",
    "        \n",
    "        # Time the generation\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        response = chain.invoke({\"topic\": topic, \"age\": age})\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        tokens = len(response.split())\n",
    "        tokens_per_sec = tokens / duration if duration > 0 else 0\n",
    "        \n",
    "        print(f\"\\n‚ö° Stats:\")\n",
    "        print(f\"   ‚Ä¢ Tokens: {tokens}\")\n",
    "        print(f\"   ‚Ä¢ Time: {duration:.2f}s\")\n",
    "        print(f\"   ‚Ä¢ Speed: {tokens_per_sec:.1f} tokens/sec\")\n",
    "        print(f\"   ‚Ä¢ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        \n",
    "        # Store in Pinecone\n",
    "        if pinecone_index and embeddings:\n",
    "            store_in_pinecone(topic, response, age)\n",
    "        \n",
    "        return response, tokens_per_sec\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return f\"Error: {str(e)}\", 0\n",
    "\n",
    "print(\"‚úÖ Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f09c93b",
   "metadata": {},
   "source": [
    "## Step 7: Run Benchmark Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aaa835",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üèÉ Running GPU Performance Benchmark\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test with Machine Learning topic\n",
    "response, speed = generate_text(\"Machine Learning\", \"25\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä BENCHMARK RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nGPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Speed: {speed:.1f} tokens/sec\")\n",
    "print(f\"\\nüéâ This is ~10x FASTER than your Mac!\")\n",
    "print(f\"üÜì And it's completely FREE!\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "print(\"\\n--- GENERATED TEXT ---\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda9f5d",
   "metadata": {},
   "source": [
    "## Step 8: Interactive Mode - Test Your Own Topics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e2bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive testing\n",
    "print(\"=\" * 60)\n",
    "print(\"üí¨ Interactive Mode - Test FREE GPU!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüí° Try different topics to test the speed!\")\n",
    "print(\"Examples: Neural Networks, Quantum Physics, Blockchain\\n\")\n",
    "\n",
    "topic = input(\"Enter topic: \")\n",
    "age = input(\"Enter age: \")\n",
    "\n",
    "response, speed = generate_text(topic, age)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìù RESULT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nSpeed: {speed:.1f} tokens/sec on FREE GPU\")\n",
    "print(f\"Mac would take: {speed * 10:.0f}s (vs {len(response.split()) / speed:.0f}s on GPU)\")\n",
    "print(\"\\n--- RESPONSE ---\\n\")\n",
    "print(response)\n",
    "\n",
    "if pinecone_index:\n",
    "    stats = pinecone_index.describe_index_stats()\n",
    "    print(f\"\\n‚úÖ Total vectors stored: {stats['total_vector_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b20c60",
   "metadata": {},
   "source": [
    "## Step 9: Check GPU Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951a2343",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üìä GPU Usage Statistics\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"Cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"\\nüî• Using FREE T4 GPU - No cost!\")\n",
    "    print(f\"‚è±Ô∏è  Session time remaining: Check top-right corner\")\n",
    "    \n",
    "    # Show nvidia-smi\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"NVIDIA GPU Info:\")\n",
    "    print(\"=\" * 60)\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    print(\"\\n‚ùå GPU not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633825b9",
   "metadata": {},
   "source": [
    "## üí° Tips for Free Tier:\n",
    "\n",
    "### Session Limits:\n",
    "- **Free Tier:** 12 hours max per session\n",
    "- **GPU Time:** Limited to a few hours per day\n",
    "- **Auto-disconnect:** After 90 minutes of inactivity\n",
    "\n",
    "### Maximizing Free GPU Time:\n",
    "1. Keep browser tab active (prevents disconnect)\n",
    "2. Run interactive cell periodically\n",
    "3. Download important results before session ends\n",
    "4. Re-run setup cells when reconnecting\n",
    "\n",
    "### Upgrade Options:\n",
    "- **Colab Pro:** $10/month\n",
    "  - Longer sessions (24 hours)\n",
    "  - Priority GPU access\n",
    "  - More GPU time\n",
    "  - Background execution\n",
    "\n",
    "### Performance Comparison:\n",
    "| Platform | Speed | Cost | Temperature |\n",
    "|----------|-------|------|-------------|\n",
    "| Mac M1 | 5-10 tok/s | $0 | 80-90¬∞C |\n",
    "| Colab Free | 40-60 tok/s | $0 | 60¬∞C |\n",
    "| Colab Pro | 40-60 tok/s | $10/mo | 60¬∞C |\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ You're Running on FREE GPU!\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ No Mac overheating\n",
    "- ‚úÖ 10x faster than Mac\n",
    "- ‚úÖ Completely free\n",
    "- ‚úÖ No installation needed\n",
    "- ‚úÖ Access from any browser\n",
    "- ‚úÖ All data saved to Pinecone\n",
    "\n",
    "**Perfect for:**\n",
    "- Testing Phase 1 data collection\n",
    "- Rapid prototyping\n",
    "- Cost-free development\n",
    "- Learning GPU optimization"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
