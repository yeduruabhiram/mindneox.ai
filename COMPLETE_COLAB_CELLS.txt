===============================================================================
MINDNEOX.AI - COMPLETE COLAB CELLS (COPY-PASTE READY)
===============================================================================
Copy each cell below into separate cells in Google Colab
Run in order: Cell 1 ‚Üí Cell 2 ‚Üí Cell 3 ‚Üí ... ‚Üí Cell 9

===============================================================================
CELL 1: Check GPU
===============================================================================

import torch
print("=" * 60)
print("üîç GPU Status Check")
print("=" * 60)
print(f"\nCUDA Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU Name: {torch.cuda.get_device_name(0)}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    print(f"CUDA Version: {torch.version.cuda}")
    print("\nüéâ FREE GPU IS READY!")
else:
    print("\n‚ùå Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU ‚Üí Save")

===============================================================================
CELL 2: Install Dependencies (3 minutes) - FIXED
===============================================================================

print("üì¶ Installing dependencies...")
print("‚è±Ô∏è  This will take about 3 minutes\n")

# Install main packages
!pip install -q llama-cpp-python langchain langchain-core langchain-community

# Install Pinecone (new package name - NOT pinecone-client!)
!pip install -q pinecone sentence-transformers transformers accelerate redis

print("\n‚úÖ All dependencies installed!")
print("‚úÖ Ready for GPU acceleration!")
print("‚úÖ Using NEW Pinecone package (not pinecone-client)")

===============================================================================
CELL 3: Download Model (2 minutes) - FIXED VERSION
===============================================================================

import os
MODEL_FILE = "Mistral-7B-Instruct-v0.3.Q4_K_M.gguf"
MODEL_URL = "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf"

if not os.path.exists(MODEL_FILE):
    print(f"üì• Downloading {MODEL_FILE}...")
    print("‚è±Ô∏è  This will take about 2 minutes (4.37 GB)\n")
    
    !wget --show-progress {MODEL_URL}
    
    if os.path.exists(MODEL_FILE):
        file_size = os.path.getsize(MODEL_FILE) / 1024**3
        print(f"\n‚úÖ Download complete! Size: {file_size:.2f} GB")
    else:
        print("\n‚ùå wget failed. Trying curl...")
        !curl -L -o {MODEL_FILE} {MODEL_URL}
        if os.path.exists(MODEL_FILE):
            file_size = os.path.getsize(MODEL_FILE) / 1024**3
            print(f"\n‚úÖ Downloaded with curl! Size: {file_size:.2f} GB")
        else:
            print("\n‚ùå Both methods failed. Try manual upload.")
else:
    file_size = os.path.getsize(MODEL_FILE) / 1024**3
    print(f"‚úÖ Model exists! Size: {file_size:.2f} GB")

print("\nüí° Model ready for GPU inference!")

===============================================================================
CELL 4: Setup Pinecone
===============================================================================

from pinecone import Pinecone
from langchain_community.embeddings import HuggingFaceEmbeddings

print("üîó Connecting to Pinecone...")

PINECONE_API_KEY = "pcsk_5A9JjS_JVvYF7aE1kieuSnTXitm1pEMdVhg2wkpijQ3hiV9aC7rZ2CurG5qRfXE9FxHLAh"
INDEX_NAME = "mindnex-responses"

try:
    pc = Pinecone(api_key=PINECONE_API_KEY)
    pinecone_index = pc.Index(INDEX_NAME)
    
    print("üî§ Loading embedding model on GPU...")
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cuda'}
    )
    
    stats = pinecone_index.describe_index_stats()
    print(f"\n‚úÖ Pinecone connected!")
    print(f"‚úÖ Vectors stored: {stats['total_vector_count']}")
    print(f"‚úÖ Embeddings on GPU: CUDA")
    
except Exception as e:
    print(f"‚ö†Ô∏è  Pinecone error: {e}")
    pinecone_index = None
    embeddings = None

===============================================================================
CELL 5: Load Model on GPU
===============================================================================

from llama_cpp import Llama
from langchain_community.llms import LlamaCpp
from langchain_core.prompts import PromptTemplate
from datetime import datetime

print("=" * 60)
print("üöÄ Loading Mistral-7B with FREE GPU")
print("=" * 60)

llm = LlamaCpp(
    model_path="Mistral-7B-Instruct-v0.3.Q4_K_M.gguf",
    n_ctx=8192,
    n_threads=2,
    n_gpu_layers=-1,  # ALL layers on GPU
    n_batch=512,
    temperature=0.7,
    top_p=0.95,
    repeat_penalty=1.2,
    max_tokens=500,
    verbose=False
)

print("\n‚úÖ Model loaded on GPU!")
print("‚úÖ Using ALL model layers on T4 GPU")
print("‚úÖ Expected speed: 40-60 tokens/sec (10x faster than Mac!)\n")

prompt_template = PromptTemplate(
    input_variables=["topic", "age"],
    template="[INST] Explain {topic} in detail for a {age} year old to understand. [/INST]"
)

chain = prompt_template | llm
print("=" * 60)

===============================================================================
CELL 6: Define Helper Functions
===============================================================================

def store_in_pinecone(topic: str, response: str, age: str):
    """Store response in Pinecone with GPU-accelerated embeddings"""
    if not pinecone_index or not embeddings:
        return None
    
    try:
        print("\nüíæ Storing in Pinecone...")
        embedding = embeddings.embed_query(response)
        
        vector_id = f"response_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{hash(topic) % 10000}"
        
        metadata = {
            'topic': topic,
            'age': int(age) if age.isdigit() else 12,
            'response': response,
            'word_count': len(response.split()),
            'character_count': len(response),
            'timestamp': datetime.now().isoformat(),
            'response_preview': response[:200],
            'device': 'cuda',
            'gpu_model': torch.cuda.get_device_name(0),
            'platform': 'Google Colab Free'
        }
        
        pinecone_index.upsert(
            vectors=[{
                'id': vector_id,
                'values': embedding,
                'metadata': metadata
            }]
        )
        
        print(f"   ‚úÖ Stored in Pinecone: {vector_id}")
        print(f"   ‚úÖ Embedding generated on GPU")
        return vector_id
    
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Error: {e}")
        return None


def generate_text(topic: str, age: str):
    """Generate text with GPU acceleration and timing"""
    try:
        print(f"\nüöÄ Generating response for: {topic}")
        print(f"üë§ Target age: {age}")
        
        start_time = datetime.now()
        response = chain.invoke({"topic": topic, "age": age})
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        tokens = len(response.split())
        tokens_per_sec = tokens / duration if duration > 0 else 0
        
        print(f"\n‚ö° Stats:")
        print(f"   ‚Ä¢ Tokens: {tokens}")
        print(f"   ‚Ä¢ Time: {duration:.2f}s")
        print(f"   ‚Ä¢ Speed: {tokens_per_sec:.1f} tokens/sec")
        print(f"   ‚Ä¢ GPU: {torch.cuda.get_device_name(0)}")
        
        if pinecone_index and embeddings:
            store_in_pinecone(topic, response, age)
        
        return response, tokens_per_sec
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return f"Error: {str(e)}", 0

print("‚úÖ Helper functions defined!")

===============================================================================
CELL 7: Run Benchmark Test
===============================================================================

print("=" * 60)
print("üèÉ Running GPU Performance Benchmark")
print("=" * 60)

response, speed = generate_text("Machine Learning", "25")

print("\n" + "=" * 60)
print("üìä BENCHMARK RESULTS")
print("=" * 60)
print(f"\nGPU: {torch.cuda.get_device_name(0)}")
print(f"Speed: {speed:.1f} tokens/sec")
print(f"\nüéâ This is ~10x FASTER than your Mac!")
print(f"üÜì And it's completely FREE!")
print("\n" + "=" * 60)

print("\n--- GENERATED TEXT ---\n")
print(response)

===============================================================================
CELL 8: Interactive Mode - Test Your Topics
===============================================================================

print("=" * 60)
print("üí¨ Interactive Mode - Test FREE GPU!")
print("=" * 60)
print("\nüí° Try different topics to test the speed!")
print("Examples: Neural Networks, Quantum Physics, Blockchain\n")

topic = input("Enter topic: ")
age = input("Enter age: ")

response, speed = generate_text(topic, age)

print("\n" + "=" * 60)
print("üìù RESULT")
print("=" * 60)
print(f"\nSpeed: {speed:.1f} tokens/sec on FREE GPU")
print(f"Mac would take: ~{len(response.split()) / 8:.0f}s (vs {len(response.split()) / speed:.0f}s on GPU)")
print("\n--- RESPONSE ---\n")
print(response)

if pinecone_index:
    stats = pinecone_index.describe_index_stats()
    print(f"\n‚úÖ Total vectors stored: {stats['total_vector_count']}")

===============================================================================
CELL 9: Check GPU Usage
===============================================================================

print("=" * 60)
print("üìä GPU Usage Statistics")
print("=" * 60)

if torch.cuda.is_available():
    print(f"\nGPU Name: {torch.cuda.get_device_name(0)}")
    print(f"Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    print(f"Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB")
    print(f"Cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB")
    print(f"\nüî• Using FREE T4 GPU - No cost!")
    print(f"‚è±Ô∏è  Session time remaining: Check top-right corner")
    
    print("\n" + "=" * 60)
    print("NVIDIA GPU Info:")
    print("=" * 60)
    !nvidia-smi
else:
    print("\n‚ùå GPU not available")

===============================================================================
DONE!
===============================================================================

You now have 9 cells ready to run in Google Colab.

Expected results:
- Cell 1: Shows T4 GPU available
- Cell 2: Installs packages (3 mins)
- Cell 3: Downloads model (2 mins)
- Cell 4: Connects to Pinecone
- Cell 5: Loads model on GPU
- Cell 6: Sets up functions
- Cell 7: Runs benchmark (~40-60 tok/s)
- Cell 8: Interactive testing
- Cell 9: GPU stats

Your Mac stays cool while Colab does the work! üéâ

===============================================================================
