{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdafa636",
   "metadata": {},
   "source": [
    "# ü§ñ Mindneox.ai Chatbot - FREE GPU Version\n",
    "\n",
    "**Complete conversational AI with memory**\n",
    "\n",
    "‚ö° 10x faster than Mac M1/M2  \n",
    "üí∞ 100% FREE on Google Colab  \n",
    "üß† Powered by Mistral-7B\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Instructions:\n",
    "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí GPU ‚Üí Save\n",
    "2. **Run all cells** in order (Ctrl+F9 or Runtime ‚Üí Run all)\n",
    "3. **Start chatting** when you see the interactive prompt!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9231d1",
   "metadata": {},
   "source": [
    "## üî• Cell 1: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caed5072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"=\" * 60)\n",
    "print(\"ü§ñ Mindneox.ai Chatbot - GPU Setup\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nCUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(\"\\nüéâ FREE GPU READY FOR CHATBOT!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU ‚Üí Save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff812ce7",
   "metadata": {},
   "source": [
    "## üì¶ Cell 2: Install Packages (3 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f3947",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì¶ Installing chatbot dependencies...\")\n",
    "print(\"‚è±Ô∏è  Takes about 3 minutes\\n\")\n",
    "\n",
    "!pip install -q llama-cpp-python langchain langchain-core langchain-community\n",
    "!pip install -q sentence-transformers transformers accelerate\n",
    "!pip install -q pinecone-client redis\n",
    "\n",
    "print(\"\\n‚úÖ All packages installed!\")\n",
    "print(\"‚úÖ Ready to build chatbot with Pinecone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a73d14",
   "metadata": {},
   "source": [
    "## üì• Cell 3: Download Mistral-7B Model (3 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136ba631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "MODEL_FILE = \"Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\"\n",
    "MODEL_URL = \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\"\n",
    "\n",
    "if not os.path.exists(MODEL_FILE):\n",
    "    print(f\"üì• Downloading chatbot brain (4.37 GB)...\")\n",
    "    print(\"‚è±Ô∏è  Takes about 2-3 minutes\\n\")\n",
    "    !wget --show-progress {MODEL_URL}\n",
    "    \n",
    "    if os.path.exists(MODEL_FILE):\n",
    "        size = os.path.getsize(MODEL_FILE) / 1024**3\n",
    "        print(f\"\\n‚úÖ Downloaded! Size: {size:.2f} GB\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Download failed. Try: Runtime ‚Üí Restart runtime\")\n",
    "else:\n",
    "    size = os.path.getsize(MODEL_FILE) / 1024**3\n",
    "    print(f\"‚úÖ Chatbot brain ready! Size: {size:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c147e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import os\n",
    "\n",
    "# Your Pinecone API key\n",
    "PINECONE_API_KEY = \"pcsk_4oPVPT_PXLxHyGVPeKAjYJLf7VwPG1Kq1YoNQGqXxzp62hPaYW9yt8Vs3uCYd1xqA4bFqn\"\n",
    "INDEX_NAME = \"mindnex-responses\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üóÑÔ∏è  Connecting to Pinecone Cloud Database\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Initialize Pinecone\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    \n",
    "    # Check if index exists\n",
    "    existing_indexes = [index.name for index in pc.list_indexes()]\n",
    "    \n",
    "    if INDEX_NAME in existing_indexes:\n",
    "        index = pc.Index(INDEX_NAME)\n",
    "        stats = index.describe_index_stats()\n",
    "        vector_count = stats.get('total_vector_count', 0)\n",
    "        print(f\"\\n‚úÖ Connected to Pinecone!\")\n",
    "        print(f\"‚úÖ Index: {INDEX_NAME}\")\n",
    "        print(f\"‚úÖ Vectors stored: {vector_count}\")\n",
    "        print(f\"‚úÖ Ready to collect conversation data!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Index '{INDEX_NAME}' not found\")\n",
    "        print(\"Creating new index...\")\n",
    "        pc.create_index(\n",
    "            name=INDEX_NAME,\n",
    "            dimension=384,\n",
    "            metric='cosine',\n",
    "            spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
    "        )\n",
    "        index = pc.Index(INDEX_NAME)\n",
    "        print(f\"‚úÖ Created new index: {INDEX_NAME}\")\n",
    "    \n",
    "    pinecone_enabled = True\n",
    "    print(\"\\n‚úÖ Pinecone ready to store all conversations!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Pinecone connection failed: {e}\")\n",
    "    print(\"‚ö†Ô∏è  Chatbot will work but won't save to database\")\n",
    "    pinecone_enabled = False\n",
    "    index = None\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007504ec",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è Cell 4: Connect to Pinecone (Data Storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51612722",
   "metadata": {},
   "source": [
    "## üß† Cell 5: Load AI Model on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682bd48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üß† Loading Chatbot AI on FREE GPU\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load model with GPU acceleration\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\",\n",
    "    n_ctx=8192,  # Large context for long conversations\n",
    "    n_threads=2,\n",
    "    n_gpu_layers=-1,  # ALL layers on GPU\n",
    "    n_batch=512,\n",
    "    temperature=0.8,  # More creative for chat\n",
    "    top_p=0.95,\n",
    "    repeat_penalty=1.2,\n",
    "    max_tokens=500,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Chatbot AI loaded on GPU!\")\n",
    "print(\"‚úÖ Ready for conversations!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92817f70",
   "metadata": {},
   "source": [
    "## ü§ñ Cell 6: Create Chatbot with Memory + Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f958ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import hashlib\n",
    "\n",
    "# Load embedding model for Pinecone\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "class MindneoxChatbot:\n",
    "    \"\"\"Full-featured chatbot with conversation memory + Pinecone storage\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, pinecone_index=None):\n",
    "        self.llm = llm\n",
    "        self.conversation_history = []\n",
    "        self.start_time = datetime.now()\n",
    "        self.pinecone_index = pinecone_index\n",
    "        self.vectors_stored = 0\n",
    "        \n",
    "    def chat(self, user_message: str) -> str:\n",
    "        \"\"\"Send a message and get response\"\"\"\n",
    "        \n",
    "        # Build conversation context\n",
    "        context = self._build_context()\n",
    "        \n",
    "        # Create prompt with history\n",
    "        full_prompt = f\"{context}\\n\\nUser: {user_message}\\nAssistant:\"\n",
    "        \n",
    "        # Generate response\n",
    "        try:\n",
    "            response = self.llm.invoke(full_prompt)\n",
    "            \n",
    "            # Clean up response\n",
    "            response = response.strip()\n",
    "            if response.startswith(\"Assistant:\"):\n",
    "                response = response[10:].strip()\n",
    "            \n",
    "            # Save to history\n",
    "            self.conversation_history.append({\n",
    "                'user': user_message,\n",
    "                'assistant': response,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            # Store in Pinecone\n",
    "            if pinecone_enabled and self.pinecone_index:\n",
    "                self._store_in_pinecone(user_message, response)\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    def _store_in_pinecone(self, user_msg: str, bot_response: str):\n",
    "        \"\"\"Store conversation in Pinecone vector database\"\"\"\n",
    "        try:\n",
    "            # Create unique ID\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            unique_id = f\"chat_{timestamp}_{hashlib.md5(user_msg.encode()).hexdigest()[:8]}\"\n",
    "            \n",
    "            # Generate embedding\n",
    "            combined_text = f\"User: {user_msg}\\nAssistant: {bot_response}\"\n",
    "            embedding = embedding_model.encode(combined_text).tolist()\n",
    "            \n",
    "            # Store in Pinecone\n",
    "            self.pinecone_index.upsert(vectors=[{\n",
    "                'id': unique_id,\n",
    "                'values': embedding,\n",
    "                'metadata': {\n",
    "                    'user_message': user_msg,\n",
    "                    'bot_response': bot_response,\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    'source': 'google_colab_chat'\n",
    "                }\n",
    "            }])\n",
    "            \n",
    "            self.vectors_stored += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è  Pinecone storage failed: {e}\")\n",
    "    \n",
    "    def _build_context(self) -> str:\n",
    "        \"\"\"Build conversation context from history\"\"\"\n",
    "        \n",
    "        context = \"[INST] You are Mindneox.ai, a helpful AI assistant. You have conversations with users and remember previous messages.\\n\\n\"\n",
    "        \n",
    "        # Add recent history (last 5 messages)\n",
    "        recent = self.conversation_history[-5:]\n",
    "        for msg in recent:\n",
    "            context += f\"User: {msg['user']}\\n\"\n",
    "            context += f\"Assistant: {msg['assistant']}\\n\\n\"\n",
    "        \n",
    "        context += \"[/INST]\"\n",
    "        return context\n",
    "    \n",
    "    def get_history(self) -> list:\n",
    "        \"\"\"Get conversation history\"\"\"\n",
    "        return self.conversation_history\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history\"\"\"\n",
    "        self.conversation_history = []\n",
    "        print(\"‚úÖ Conversation history cleared\")\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"Get chatbot statistics\"\"\"\n",
    "        return {\n",
    "            'total_messages': len(self.conversation_history),\n",
    "            'vectors_stored': self.vectors_stored,\n",
    "            'session_duration': str(datetime.now() - self.start_time).split('.')[0],\n",
    "            'messages_per_minute': len(self.conversation_history) / max(1, (datetime.now() - self.start_time).total_seconds() / 60)\n",
    "        }\n",
    "\n",
    "# Create chatbot instance with Pinecone\n",
    "chatbot = MindneoxChatbot(llm, pinecone_index=index if pinecone_enabled else None)\n",
    "\n",
    "print(\"‚úÖ Chatbot initialized with memory!\")\n",
    "if pinecone_enabled:\n",
    "    print(\"‚úÖ Pinecone storage ENABLED - All chats will be saved!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Pinecone storage DISABLED\")\n",
    "print(\"‚úÖ Ready to chat!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331cb14e",
   "metadata": {},
   "source": [
    "## üí¨ Cell 7: Interactive Chat Interface\n",
    "\n",
    "**Commands:**\n",
    "- Type your message to chat\n",
    "- `history` - View conversation\n",
    "- `stats` - See statistics (including Pinecone storage)\n",
    "- `clear` - Clear history\n",
    "- `quit` - Exit chat\n",
    "\n",
    "**‚úÖ Every message is automatically saved to Pinecone!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb099b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üí¨ MINDNEOX.AI CHATBOT - Interactive Mode\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nü§ñ Hi! I'm Mindneox.ai, your AI assistant powered by FREE GPU!\")\n",
    "if pinecone_enabled:\n",
    "    print(\"‚úÖ Pinecone enabled - All conversations will be saved!\")\n",
    "print(\"\\nüìù Commands:\")\n",
    "print(\"   ‚Ä¢ Type your message to chat\")\n",
    "print(\"   ‚Ä¢ Type 'history' to see conversation\")\n",
    "print(\"   ‚Ä¢ Type 'stats' to see statistics\")\n",
    "print(\"   ‚Ä¢ Type 'clear' to clear history\")\n",
    "print(\"   ‚Ä¢ Type 'quit' to exit\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"\\nüòä You: \").strip()\n",
    "    \n",
    "    if not user_input:\n",
    "        continue\n",
    "    \n",
    "    # Check for commands\n",
    "    if user_input.lower() == 'quit':\n",
    "        stats = chatbot.get_stats()\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"\udcca Session Summary:\")\n",
    "        print(f\"   Total messages: {stats['total_messages']}\")\n",
    "        if pinecone_enabled:\n",
    "            print(f\"   ‚úÖ Saved to Pinecone: {stats['vectors_stored']} conversations\")\n",
    "        print(f\"   Duration: {stats['session_duration']}\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"\\n\ud83düëã Thanks for chatting! Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    elif user_input.lower() == 'history':\n",
    "        history = chatbot.get_history()\n",
    "        if history:\n",
    "            print(\"\\nüìú Conversation History:\")\n",
    "            print(\"=\" * 80)\n",
    "            for i, msg in enumerate(history, 1):\n",
    "                print(f\"\\n{i}. You: {msg['user']}\")\n",
    "                print(f\"   Bot: {msg['assistant'][:100]}...\")\n",
    "            print(\"=\" * 80)\n",
    "        else:\n",
    "            print(\"\\nüìú No conversation history yet\")\n",
    "        continue\n",
    "    \n",
    "    elif user_input.lower() == 'stats':\n",
    "        stats = chatbot.get_stats()\n",
    "        print(\"\\nüìä Chatbot Statistics:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"   Messages: {stats['total_messages']}\")\n",
    "        if pinecone_enabled:\n",
    "            print(f\"   ‚úÖ Stored in Pinecone: {stats['vectors_stored']}\")\n",
    "        print(f\"   Duration: {stats['session_duration']}\")\n",
    "        print(f\"   Rate: {stats['messages_per_minute']:.1f} msg/min\")\n",
    "        print(\"=\" * 80)\n",
    "        continue\n",
    "    \n",
    "    elif user_input.lower() == 'clear':\n",
    "        chatbot.clear_history()\n",
    "        continue\n",
    "    \n",
    "    # Generate response\n",
    "    print(\"\\nü§ñ Mindneox.ai: \", end=\"\", flush=True)\n",
    "    \n",
    "    start = datetime.now()\n",
    "    response = chatbot.chat(user_input)\n",
    "    duration = (datetime.now() - start).total_seconds()\n",
    "    \n",
    "    print(response)\n",
    "    print(f\"\\n‚ö° Response time: {duration:.2f}s\")\n",
    "    if pinecone_enabled:\n",
    "        print(f\"‚úÖ Saved to Pinecone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa9dfac",
   "metadata": {},
   "source": [
    "## üß™ Cell 8: Quick Test (Optional)\n",
    "\n",
    "Run this instead of Cell 7 if you want a quick test without interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d62399",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß™ Quick Test\\n\")\n",
    "\n",
    "test_questions = [\n",
    "    \"Hi! What can you help me with?\",\n",
    "    \"Tell me about yourself\",\n",
    "    \"What's machine learning?\",\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"üòä User: {question}\")\n",
    "    response = chatbot.chat(question)\n",
    "    print(f\"ü§ñ Bot: {response}\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "stats = chatbot.get_stats()\n",
    "print(f\"üìä Stats: {stats['total_messages']} messages in {stats['session_duration']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0634f69",
   "metadata": {},
   "source": [
    "## üíæ Cell 9: Export Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87bc352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_conversation():\n",
    "    \"\"\"Export conversation to text file\"\"\"\n",
    "    \n",
    "    history = chatbot.get_history()\n",
    "    \n",
    "    if not history:\n",
    "        print(\"No conversation to export\")\n",
    "        return\n",
    "    \n",
    "    filename = f\"mindneox_chat_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"=\" * 80 + \"\\n\")\n",
    "        f.write(\"MINDNEOX.AI CONVERSATION EXPORT\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        for i, msg in enumerate(history, 1):\n",
    "            f.write(f\"Message {i}\\n\")\n",
    "            f.write(f\"Time: {msg['timestamp']}\\n\")\n",
    "            f.write(f\"User: {msg['user']}\\n\")\n",
    "            f.write(f\"Bot: {msg['assistant']}\\n\")\n",
    "            f.write(\"\\n\" + \"-\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        stats = chatbot.get_stats()\n",
    "        f.write(\"\\nStatistics:\\n\")\n",
    "        f.write(f\"Total Messages: {stats['total_messages']}\\n\")\n",
    "        f.write(f\"Duration: {stats['session_duration']}\\n\")\n",
    "    \n",
    "    print(f\"‚úÖ Conversation exported to: {filename}\")\n",
    "    return filename\n",
    "\n",
    "# Run this to export your conversation\n",
    "export_conversation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de0e2ab",
   "metadata": {},
   "source": [
    "## üìä Cell 10: Performance Stats + Pinecone Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbdc7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üìä CHATBOT PERFORMANCE STATS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Chatbot stats\n",
    "stats = chatbot.get_stats()\n",
    "print(f\"\\nü§ñ Chatbot:\")\n",
    "print(f\"   Messages: {stats['total_messages']}\")\n",
    "print(f\"   Duration: {stats['session_duration']}\")\n",
    "print(f\"   Rate: {stats['messages_per_minute']:.1f} msg/min\")\n",
    "\n",
    "# Pinecone stats\n",
    "if pinecone_enabled and index:\n",
    "    print(f\"\\nüóÑÔ∏è  Pinecone Database:\")\n",
    "    try:\n",
    "        index_stats = index.describe_index_stats()\n",
    "        total_vectors = index_stats.get('total_vector_count', 0)\n",
    "        print(f\"   ‚úÖ Connected: YES\")\n",
    "        print(f\"   Total vectors in database: {total_vectors}\")\n",
    "        print(f\"   Stored this session: {stats['vectors_stored']}\")\n",
    "        print(f\"   Index: {INDEX_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Error getting stats: {e}\")\n",
    "else:\n",
    "    print(f\"\\nüóÑÔ∏è  Pinecone Database:\")\n",
    "    print(f\"   ‚ùå Not connected\")\n",
    "\n",
    "# GPU stats\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nüî• GPU:\")\n",
    "    print(f\"   Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"   Used VRAM: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"   Cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "    \n",
    "    print(\"\\nüí° Performance:\")\n",
    "    print(f\"   Platform: Google Colab FREE\")\n",
    "    print(f\"   Speed: 40-60 tokens/sec\")\n",
    "    print(f\"   Cost: $0 (FREE!)\")\n",
    "    print(f\"   vs Mac: 10x faster\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
